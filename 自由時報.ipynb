{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'http://news.ltn.com.tw/list/breakingnews/business'\n",
    "html = requests.get(url)\n",
    "sp = BeautifulSoup(html.text, 'html.parser')\n",
    "\n",
    "data1 = sp.select(\"a.tit\")\n",
    "\n",
    "for i in data1:\n",
    "    print(i['href'])\n",
    "\n",
    "\n",
    "tit  =sp.select(\"title\")\n",
    "tit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import re\n",
    "url = 'http://news.ltn.com.tw/news/politics/breakingnews/2334121'\n",
    "html = requests.get(url)\n",
    "sp = BeautifulSoup(html.text, 'html.parser')\n",
    "\n",
    "data1 = sp.select(\"div.whitecon.articlebody h1\")[0].text\n",
    "\n",
    "data2=re.findall('[^\\n]+',data1)\n",
    "\n",
    "data3 = sp.select(\"div.text span\")[0].text\n",
    "\n",
    "\n",
    "data4 =sp.select(\"div.text p\")\n",
    "\n",
    "data5=''\n",
    "for i in range(len(data4)):\n",
    "    data5 += data4[i].text\n",
    "\n",
    "data6=re.search(r'〔.+〕',data5)\n",
    "\n",
    "\n",
    "data6.group()\n",
    "\n",
    "tit  =sp.select(\"title\")\n",
    "\n",
    "\n",
    "data6\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business4不存在\n",
      "world4不存在\n",
      "politics4不存在\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>標題</th>\n",
       "      <th>版別</th>\n",
       "      <th>日期時間</th>\n",
       "      <th>文章內容</th>\n",
       "      <th>文章來源</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [標題, 版別, 日期時間, 文章內容, 文章來源]\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 完成版\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "a1=[]\n",
    "a2=[]\n",
    "a3=[]\n",
    "a4=[]\n",
    "a5=[]\n",
    "a6=[]\n",
    "a7=[]\n",
    "\n",
    "board=['business','world','politics']\n",
    "#board=['society']\n",
    "\n",
    "\n",
    "for a in board:\n",
    "#     for i in range(1,5):\n",
    "        try:\n",
    "            url = 'http://news.ltn.com.tw/list/breakingnews'\n",
    "            html = requests.get(url)\n",
    "            sp = BeautifulSoup(html.text, 'html.parser')\n",
    "            data1 = sp.select(\"a.tit\")\n",
    "            tit  =sp.select(\"a.tit span\")\n",
    "            for j in data1:\n",
    "                next_web='http:'+j['href']\n",
    "                html1 = requests.get(next_web)\n",
    "                sp1 = BeautifulSoup(html1.text, 'html.parser')\n",
    "                data11 = sp1.select(\"div.whitecon.articlebody h1\")  \n",
    "                if len(data11)!=0:\n",
    "                    data111 = sp1.select(\"div.whitecon.articlebody h1\")[0].text  \n",
    "                    data2=re.findall('[^\\n]+',data111) #標題\n",
    "                    print(data2)\n",
    "                    data3 = sp1.select(\"div.text span\")[0].text #時間\n",
    "                    data4 =sp1.select(\"div.text p\")\n",
    "\n",
    "                    data5=''  #內文\n",
    "                    for i in range(len(data4)):\n",
    "                        data5 += data4[i].text\n",
    "\n",
    "                    source='自由時報'\n",
    "\n",
    "                    a1.append(data2)\n",
    "                    a2.append(a)\n",
    "                    a3.append(data3)\n",
    "                    a5.append(data5)\n",
    "                    a6.append(source)\n",
    "        except:\n",
    "            print(a+str(i)+'不存在')\n",
    "            \n",
    "aa= {\"標題\":a1,\"版別\":a2,\"日期時間\":a3,\"文章內容\":a5,\"文章來源\":a6}\n",
    "\n",
    "aa_df=pd.DataFrame(aa, columns=[\"標題\",\"版別\",\"日期時間\",\"文章內容\",\"文章來源\"])\n",
    "\n",
    "aa_df.to_csv(\"/Users/adam/Desktop/py_output/news_crawl/freedom_0.csv\")\n",
    "\n",
    "aa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147        ['可惡！疑貪善款發「地震財」 台南2里長被收押']\n",
      "182       ['身心障礙女遭詐騙集團利用 還被人拐到中國結婚…']\n",
      "254            ['網戀英國醫師 竹科貴婦被騙千萬慘離婚']\n",
      "266        ['向當事人宣稱「花30萬能買通法官」 律師慘了']\n",
      "268         ['海巡中校詐領檢舉獎金737萬元 判刑10年']\n",
      "270           ['涉圖利新北市議員公司 清潔隊員羈押禁見']\n",
      "299     ['12天賺2900萬！他26歲設網站賣金礦 但下場…']\n",
      "323       ['貴婦奈奈10億金流曝光！ 週刊爆坐擁內湖5豪宅']\n",
      "362    ['高市府前專委趙嘉寶貪污棄保潛逃5年 遭列要案查緝專刊']\n",
      "377               ['貪官葉世文 再判刑10年半定讞']\n",
      "Name: 標題, dtype: object\n",
      "['檢調獲報後依法偵辦並將兩位里長收押', '檢調獲報後依法偵辦並將兩位里長收押', '請廠商高報工程費']\n",
      "['阮女成為禁治產人是信用卡消費後3年', '中度身心障礙的阮女常遭詐騙集團利用', '新北地院板橋簡易庭以請求權罹於時效判銀行敗訴']\n",
      "['匯款上千萬元給「檸檬」交保證金', '張婦誤認戀人「檸檬」身陷監牢', '證實張婦曾寄出多張提款卡、手機']\n",
      "['承辦檢察官收案後傳喚多名曾委任姚姓律師的當事人作證', '日前依詐欺罪起訴姚姓律師', '發現姚就是「司法黃牛」']\n",
      "['沈大祥等人因長年經辦查緝載運私菸案件', '1件職務詐財罪判刑7年、褫奪公權2年', '詐領檢舉8件私菸、私酒獎金']\n",
      "['新北地院今裁定徐男等3人羈押禁見', '新北地院當時裁定陳男羈押禁見', '訊後聲押禁見陳男、徐男及陳男公司的蔡、張、顏姓3名員工共5人']\n",
      "['固定12天收益600元「小黃金」', '每12天固定可挖600元（小黃金）', '然後游嫌以現金領走2890萬元']\n",
      "['而目前貴婦奈奈和家人卻被檢調查出藏有5戶豪宅', '（圖擷取自臉書）〔即時新聞／綜合報導〕部落客貴婦奈奈（本名蘇陳端）與丈夫、公公等3人', '貴婦奈奈和家人逃亡前在內湖重劃區購入5間豪宅']\n",
      "['涉貪遭通緝多年的趙嘉寶（左）近日擠身刑事局「重要緊急查緝專案之列', '高雄市刑大近日於是將趙嘉寶提報名列刑事局重要緊急查緝專案（俗稱「要案查緝專刊」）', '之後並到處「趴趴走」還在臉書打卡']\n",
      "['一審以葉涉林口A7、桃園八德合宜宅案、財產來源不明罪共判19年', '高院更一審將桃園八德合宜住宅收賄案判刑7年半', '高院更一審將桃園八德合宜住宅收賄案判刑7年半']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from snownlp import SnowNLP\n",
    "MD=pd.read_csv(\"/Users/adam/Desktop/py_output/news_crawl/freedom.csv\")  #讀取檔案\n",
    "\n",
    "ss=MD[MD['文章內容'].str.contains('洗錢',regex=True)|MD['文章內容'].str.contains('貪污',regex=True)|MD['文章內容'].str.contains('資恐',regex=True)|MD['文章內容'].str.contains('吸金',regex=True)|MD['文章內容'].str.contains('收賄',regex=True)]\n",
    "\n",
    "\n",
    "print(ss['標題'])\n",
    "\n",
    "for i in ss['文章內容']:\n",
    "    s = SnowNLP(i)\n",
    "    w=SnowNLP(s.han)\n",
    "    print(s.summary(3))\n",
    "\n",
    "ss.to_csv(\"/Users/adam/Desktop/py_output/news_crawl/freedom_bad.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

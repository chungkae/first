{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HRkNiTXRyw-c"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.metrics import  BigramAssocMeasures\n",
    "from nltk.probability import  FreqDist,ConditionalFreqDist\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import re\n",
    "import sklearn\n",
    "from nltk.classify.scikitlearn import  SklearnClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import  MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import  LogisticRegression\n",
    "from sklearn.metrics import  accuracy_score\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "fRb7CzL3Wtfh",
    "outputId": "f3376c5d-5ecd-44fa-deb1-fbd135654886"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /Users/adam/Jupyter/TextMining/dict.txt.big.txt ...\n",
      "Loading model from cache /var/folders/sf/2gqbqkg57fxb0rf8m_kdk4z80000gn/T/jieba.u501f071ef6c2eb8ce82a25c715304e45.cache\n",
      "Loading model cost 1.142 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# 利用新聞內容,做新聞分類屬性預測\n",
    "\n",
    "# 先斷詞 -- 先用Jieba繁體字庫\n",
    "\n",
    "df = pd.read_csv('freedom.csv',encoding=\"cp950\")  #讀取檔案 CSV須加編碼\n",
    "\n",
    "world = df[df[\"版別\"] == \"world\"]['文章內容']\n",
    "politics = df[df[\"版別\"] == \"politics\"]['文章內容']\n",
    "\n",
    "jieba.set_dictionary('dict.txt.big.txt')   #設定結巴的繁體字典 (中研院)\n",
    "jieba.load_userdict(\"userdict.txt\")  #可以補充字典\n",
    "\n",
    "#目標型態是[['','',''],['','',''],['','','']...]\n",
    "\n",
    "wor = []\n",
    "for article in world:\n",
    "    wor_2 = []\n",
    "    newcontent = re.sub(\"〔.*?〕|（.*?）\",'',str(article))       #*?獲取最短滿足條件 > 這邊因為資料內容，把記者XXX拿掉\n",
    "    newcontent_2 = re.sub(\"\\d|[\\s+\\.\\!\\/_,$%^*(+\\\"\\'《》「」]+|[+-—！，。？、~@#￥%……&*()（）:]+\",'',newcontent) #拿掉停止詞或符號\n",
    "    words = jieba.cut(newcontent_2,cut_all=False)\n",
    "    for w in words:\n",
    "        wor_2.append(w)\n",
    "        wor.append(wor_2)\n",
    "    \n",
    "pol = []\n",
    "for article in politics:\n",
    "    pol_2 = []\n",
    "    newcontent = re.sub(\"〔.*?〕|（.*?）\",'',str(article))       \n",
    "    newcontent_2 = re.sub(\"\\d|[\\s+\\.\\!\\/_,$%^*(+\\\"\\'《》「」]+|[+-—！，。？、~@#￥%……&*()（）:]+\",'',newcontent)\n",
    "    words = jieba.cut(newcontent_2,cut_all=False)\n",
    "    for w in words:\n",
    "        pol_2.append(w)    \n",
    "        pol.append(pol_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MSXtnKwack4Y"
   },
   "outputs": [],
   "source": [
    "# 計算斷完詞後的詞頻 -> 後續用來計算資訊量\n",
    "\n",
    "word_fd = FreqDist() #統計詞頻\n",
    "cond_word_fd = ConditionalFreqDist() #統計在條件下的詞頻\n",
    "\n",
    "#預計結果會是dict {'詞':num,...}\n",
    "for word2 in wor:\n",
    "    for word in word2:\n",
    "        word_fd[word] += 1\n",
    "        cond_word_fd['wor'][word] += 1\n",
    "\n",
    "for word2 in pol:\n",
    "    for word in word2:\n",
    "        word_fd[word] += 1\n",
    "        cond_word_fd['pol'][word] += 1\n",
    "\n",
    "wor_word_count = cond_word_fd['wor'].N() #國際新聞的詞量\n",
    "pol_word_count = cond_word_fd['pol'].N() #政治新聞的詞量\n",
    "\n",
    "total_word_count = wor_word_count + pol_word_count  #總詞量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LDqAShfuei8o"
   },
   "outputs": [],
   "source": [
    "word_scores={}\n",
    "\n",
    "for word, freq in word_fd.items():   #分別計算在各詞在不同版別的訊息量 (這邊用卡方;也可以用其他的)\n",
    "    wor_score = BigramAssocMeasures.chi_sq(cond_word_fd['wor'][word],  (freq, wor_word_count), total_word_count)\n",
    "    pol_score = BigramAssocMeasures.chi_sq(cond_word_fd['pol'][word],  (freq, pol_word_count), total_word_count)\n",
    "    word_scores[word] = wor_score + pol_score #各個詞總訊息量 (等於分別相加)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O9n_cTYei3zh"
   },
   "outputs": [],
   "source": [
    "\n",
    "#按照訊息量排序(大->小) [只留訊息前幾大的詞]\n",
    "best_vals = sorted(word_scores.items(), key=lambda x:x[1],  reverse=True)[:300]\n",
    "# set 去除重複\n",
    "best_words = set([w for w,s in best_vals])\n",
    "ss=dict([(word, True) for word in best_words])\n",
    "\n",
    "#產生訓練用樣本\n",
    "worfeature = []\n",
    "for items in wor:\n",
    "    a = {}\n",
    "    for item in items:\n",
    "        if item in ss.keys():  #前幾大訊息量的詞語\n",
    "            a[item]='True'\n",
    "    worWords = [a,'wor'] # 型態:一篇新聞一個詞語的dict + 'wor'  [ [dict,'wor'],[dict,'wor']..... ]\n",
    "    worfeature.append(worWords) \n",
    "\n",
    "polfeature = []\n",
    "for items in pol:\n",
    "    a = {}\n",
    "    for item in items:\n",
    "        if item in ss.keys():\n",
    "            a[item]='True'\n",
    "    polWords = [a,'pol'] \n",
    "    polfeature.append(polWords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LbQYUutclNmW"
   },
   "outputs": [],
   "source": [
    "#隨機排序 樣本數須控制一樣\n",
    "shuffle(polfeature)\n",
    "shuffle(worfeature)\n",
    "size = int(len(polfeature)*0.3)\n",
    "\n",
    "train =  polfeature[size:]+worfeature[size:]\n",
    "test = polfeature[:size]+worfeature[:size]\n",
    "\n",
    "data,tag = zip(*test) #把x與y拆開"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ODrJqQ_n2Hd"
   },
   "outputs": [],
   "source": [
    "def score(ml):\n",
    "    classifier = SklearnClassifier(ml) \n",
    "    classifier.train(train) #訓練\n",
    "    pred = classifier.classify_many(data) #預測結果\n",
    "    n = 0\n",
    "    s = len(pred)\n",
    "    for i in range(0,s):\n",
    "        if pred[i]==tag[i]:\n",
    "            n = n+1\n",
    "    return n/s #準確度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 734
    },
    "colab_type": "code",
    "id": "ll6r5gfmpzaq",
    "outputId": "52118d76-2a0a-47a0-9f66-1a48c4273ae8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------naive_bayes-------\n",
      "樸素貝葉斯-伯努利      {alpha=0.1}, accuracy is 0.982456\n",
      "樸素貝葉斯-Multinomial {alpha=0.1}, accuracy is 0.991228\n",
      "樸素貝葉斯-伯努利      {alpha=1}, accuracy is 0.982456\n",
      "樸素貝葉斯-Multinomial {alpha=1}, accuracy is 0.986842\n",
      "樸素貝葉斯-伯努利      {alpha=10}, accuracy is 0.960526\n",
      "樸素貝葉斯-Multinomial {alpha=10}, accuracy is 0.986842\n",
      "樸素貝葉斯-伯努利      {alpha=100}, accuracy is 0.820175\n",
      "樸素貝葉斯-Multinomial {alpha=100}, accuracy is 0.973684\n",
      "\n",
      "\n",
      "--------Logistic regression-------\n",
      "Logistic {solver=newton-cg}, accuracy is  0.991228\n",
      "Logistic {solver=lbfgs}, accuracy is  0.991228\n",
      "Logistic {solver=sag}, accuracy is  0.991228\n",
      "Logistic {solver=liblinear}, accuracy is  0.991228\n",
      "\n",
      "\n",
      "--------SVM-------\n",
      "SVM {懲罰項=0.001, gamma=scale}, accuracy is 0.929825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM {懲罰項=0.001, gamma=auto}, accuracy is 0.899123\n",
      "SVM {懲罰項=0.001, gamma=0.0001}, accuracy is 0.899123\n",
      "SVM {懲罰項=0.001, gamma=0.001}, accuracy is 0.899123\n",
      "SVM {懲罰項=0.001, gamma=0.1}, accuracy is 0.767544\n",
      "SVM {懲罰項=0.1, gamma=scale}, accuracy is 0.956140\n",
      "SVM {懲罰項=0.1, gamma=auto}, accuracy is 0.899123\n",
      "SVM {懲罰項=0.1, gamma=0.0001}, accuracy is 0.899123\n",
      "SVM {懲罰項=0.1, gamma=0.001}, accuracy is 0.899123\n",
      "SVM {懲罰項=0.1, gamma=0.1}, accuracy is 0.828947\n",
      "SVM {懲罰項=1, gamma=scale}, accuracy is 0.995614\n",
      "SVM {懲罰項=1, gamma=auto}, accuracy is 0.978070\n",
      "SVM {懲罰項=1, gamma=0.0001}, accuracy is 0.899123\n",
      "SVM {懲罰項=1, gamma=0.001}, accuracy is 0.960526\n",
      "SVM {懲罰項=1, gamma=0.1}, accuracy is 0.982456\n",
      "SVM {懲罰項=10, gamma=scale}, accuracy is 0.991228\n",
      "SVM {懲罰項=10, gamma=auto}, accuracy is 0.991228\n",
      "SVM {懲罰項=10, gamma=0.0001}, accuracy is 0.960526\n",
      "SVM {懲罰項=10, gamma=0.001}, accuracy is 0.986842\n",
      "SVM {懲罰項=10, gamma=0.1}, accuracy is 0.991228\n"
     ]
    }
   ],
   "source": [
    "# test 資料精準度 [分類正確率]  -- gridsearch還需轉換，這邊自己簡單試\n",
    "print('--------naive_bayes-------')\n",
    "\n",
    "for alpha in [0.1,1,10,100]: #測試alpha平滑參數\n",
    "    print('樸素貝葉斯-伯努利      {alpha=%s}, accuracy is %f'  %(alpha,score(BernoulliNB(alpha=alpha))))\n",
    "    print('樸素貝葉斯-Multinomial {alpha=%s}, accuracy is %f'  %(alpha,score(MultinomialNB(alpha=alpha))))\n",
    "\n",
    "print('\\n')\n",
    "print('--------Logistic regression-------')\n",
    "\n",
    "for solver in ['newton-cg','lbfgs','sag','liblinear']: #損失函數優化方式\n",
    "    print('Logistic {solver=%s}, accuracy is  %f' %(solver,score(LogisticRegression(solver=solver))))\n",
    "\n",
    "print('\\n')\n",
    "print('--------SVM-------')\n",
    "\n",
    "for C in [0.001,0.1,1,10]: #懲罰項;調和誤差跟分錯 -> 太大容易過擬和 (default 1)\n",
    "    for gamma in ['scale','auto',0.0001,0.001,0.1]: #gamma值大 資料影響的範圍越小 -> 可能過擬合 (default scale)\n",
    "        print('SVM {懲罰項=%s, gamma=%s}, accuracy is %f'  %(C,gamma,score(SVC(C=C,gamma=gamma,random_state=666))))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tm_emotion.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
